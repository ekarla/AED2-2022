{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ekarla/AED2-2022/blob/main/PI3_BGD_2022_02_ELENKARLA_LEIDE_MARINA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bc529c7d"
      },
      "source": [
        " <img src=\"https://drive.google.com/uc?id=18x0Fa9XWHlnH5OWkZ-UMrJVQCdsEmYQw\" width=800/>\n",
        "\n",
        "## Big Data - Projeto de Implementação 3 [2022/02] </br> \n",
        "### Dupla: Elenkarla Silva / Leide Marina"
      ],
      "id": "bc529c7d"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "x2vX9pc5ccmQ"
      },
      "outputs": [],
      "source": [
        "!wget -q https://raw.githubusercontent.com/ekarla/ProjetoBigData/main/train.csv"
      ],
      "id": "x2vX9pc5ccmQ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "AdjA_6IybFRy"
      },
      "outputs": [],
      "source": [
        "GCP_REGION = \"us-central1\"\n",
        "GCP_ZONE = \"us-central1-c\"\n",
        "\n",
        "PROJECT_ID = \"ufam-bgd-2022-02-lmsf-evs\"\n",
        "PROJECT_NUMBER = \"1076753341179\"\n",
        "PROJECT_NAME = \"ufam-bgd-2022-02-lmsf-evs\"\n",
        "\n",
        "CLUSTER_NAME = \"leide-elenkarla\"\n",
        "\n",
        "BUCKET_NAME = \"lmsf-ufam-bucket-1\"\n",
        "BUCKET_URL = f\"gs://{BUCKET_NAME}\"\n",
        "\n",
        "NOME_ARQUIVO_SCRIPT = \"clusteringtext.py\"\n",
        "NOME_ARQUIVO_ENTRADA = \"train.csv\"\n",
        "CAMINHO_ARQUIVO_ENTRADA = f\"gs://{BUCKET_URL}/{NOME_ARQUIVO_ENTRADA}\""
      ],
      "id": "AdjA_6IybFRy"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IN3yIyHLbari"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "if \"google.colab\" in sys.modules:\n",
        "    from google.colab import auth\n",
        "    \n",
        "    auth.authenticate_user()"
      ],
      "id": "IN3yIyHLbari"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FdTJKpy5bh4m"
      },
      "outputs": [],
      "source": [
        "#!gsutil mb -c standard -l $GCP_REGION -p $PROJECT_ID $BUCKET_URL\n",
        "!gsutil cp $NOME_ARQUIVO_ENTRADA $BUCKET_URL/"
      ],
      "id": "FdTJKpy5bh4m"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F0wCGJJsbofb"
      },
      "outputs": [],
      "source": [
        "!gcloud dataproc clusters create $CLUSTER_NAME --project=$PROJECT_NAME --region=$GCP_REGION --subnet default --zone=$GCP_ZONE --master-machine-type n1-standard-1 \\\n",
        " --master-boot-disk-size 32 \\\n",
        " --num-workers 5 \\\n",
        " --worker-machine-type n1-standard-1 \\\n",
        " --worker-boot-disk-size 32 \\\n",
        " --image-version 1.4-ubuntu18 \\\n",
        " --optional-components ANACONDA,JUPYTER \\\n",
        " --bucket=$BUCKET_NAME\n"
      ],
      "id": "F0wCGJJsbofb"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1JpMGaHobykB"
      },
      "outputs": [],
      "source": [
        "%%writefile $NOME_ARQUIVO_SCRIPT\n",
        "\n",
        "from pyspark import *\n",
        "from pyspark.sql import *\n",
        "from pyspark.sql.functions import split, col\n",
        "from pyspark.sql.functions import monotonically_increasing_id\n",
        "import pandas as pd\n",
        "from pyspark.ml.feature import Tokenizer\n",
        "from pyspark.ml.feature import StopWordsRemover\n",
        "from pyspark import SparkContext\n",
        "from pyspark.ml.feature import CountVectorizer\n",
        "from pyspark.ml.feature import IDF\n",
        "from pyspark.ml.feature import HashingTF\n",
        "from pyspark.ml.clustering import KMeans\n",
        "from pyspark.ml.evaluation import ClusteringEvaluator\n",
        "\n",
        "spark = SparkSession.builder.appName('LaboratorioPBD').getOrCreate()\n",
        "\n",
        "def dataframe_token(df,tipo):\n",
        "  sc =SparkContext.getOrCreate()\n",
        "  locale = sc._jvm.java.util.Locale\n",
        "  locale.setDefault(locale.forLanguageTag(\"en-US\"))\n",
        "  \n",
        "  reviews = df.select(\"uid\",tipo)\n",
        "  tokenizer = Tokenizer(inputCol=tipo, outputCol=\"tokens\")\n",
        "  reviews = tokenizer.transform(reviews)\n",
        "  swremover = StopWordsRemover(inputCol=\"tokens\", outputCol=\"words\")\n",
        "  reviews = swremover.transform(reviews)\n",
        "\n",
        "  return reviews\n",
        "\n",
        "def cria_contador_de_ocorrencia(dataframe):\n",
        "  cv = CountVectorizer(inputCol=\"words\", outputCol=\"rawFeatures\", vocabSize = 100)\n",
        "  cvmodel = cv.fit(dataframe)\n",
        "  feat_reviews = cvmodel.transform(dataframe)\n",
        "  return feat_reviews\n",
        "\n",
        "def calcula_valores_TF_IDF(dataframe):\n",
        "  idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
        "  idfModel = idf.fit(dataframe)\n",
        "  dataframe = idfModel.transform(dataframe)\n",
        "  return dataframe\n",
        "\n",
        "rawdata = spark.read.load(\"gs://lmsf-ufam-bucket-1/train.csv\", format=\"csv\", header=True)\n",
        "\n",
        "rawdata = rawdata.withColumn('texto', split(rawdata['tweet'], '#').getItem(0)).withColumn('hashtag', split(rawdata['tweet'], '#').getItem(1))\n",
        "\n",
        "rawdata = rawdata.fillna({\"tweet\":\"\"})\n",
        "rawdata = rawdata.fillna({\"texto\":\"\"})\n",
        "rawdata = rawdata.fillna({\"hashtag\":\"\"})\n",
        "rawdata = rawdata.withColumn(\"uid\", monotonically_increasing_id())\n",
        "\n",
        "\n",
        "\n",
        "reviews_texto = dataframe_token(rawdata,\"texto\")\n",
        "\n",
        "feat_reviews_texto = cria_contador_de_ocorrencia(reviews_texto)\n",
        "feat_reviews_texto = calcula_valores_TF_IDF(feat_reviews_texto)\n",
        "\n",
        "reviews = dataframe_token(rawdata,\"tweet\")\n",
        "\n",
        "feat_reviews = cria_contador_de_ocorrencia(reviews)\n",
        "feat_reviews = calcula_valores_TF_IDF(feat_reviews)\n",
        "\n",
        "reviews_hashtag = dataframe_token(rawdata,\"hashtag\")\n",
        "\n",
        "feat_reviews_hashtag = cria_contador_de_ocorrencia(reviews_hashtag)\n",
        "feat_reviews_hashtag = calcula_valores_TF_IDF(feat_reviews_hashtag)\n",
        "\n",
        "\n",
        "kmeans =KMeans(featuresCol= \"features\", k=2)\n",
        "\n",
        "modelo_v1 = kmeans.fit(feat_reviews_texto)\n",
        "predictions_v1 = modelo_v1.transform(feat_reviews_texto)\n",
        "predictions_v1.select(\"uid\",\"texto\",\"prediction\").limit(5).toPandas()\n",
        "\n",
        "modelo_v2 = kmeans.fit(feat_reviews_hashtag)\n",
        "predictions_v2 = modelo_v2.transform(feat_reviews_hashtag)\n",
        "predictions_v2.select(\"uid\",\"hashtag\",\"prediction\").limit(5).toPandas()\n",
        "\n",
        "modelo_v3 = kmeans.fit(feat_reviews)\n",
        "predictions_v3 = modelo_v3.transform(feat_reviews)\n",
        "predictions_v3.select(\"uid\",\"tweet\",\"prediction\").limit(5).toPandas()\n",
        "\n",
        "evaluator = ClusteringEvaluator(predictionCol='prediction', featuresCol='features', \\\n",
        "                                metricName='silhouette', distanceMeasure='cosine')\n",
        "\n",
        "silhouette1 = evaluator.evaluate(predictions_v1)\n",
        "silhouette2 = evaluator.evaluate(predictions_v2)\n",
        "silhouette3 = evaluator.evaluate(predictions_v3)\n",
        "\n",
        "print(\"Silhouette with squared euclidean distance model v1 = \" + str(silhouette1))\n",
        "print(\"Silhouette with squared euclidean distance model v2 = \" + str(silhouette2))\n",
        "print(\"Silhouette with squared euclidean distance model v3 = \" + str(silhouette3))\n"
      ],
      "id": "1JpMGaHobykB"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NfUS6KWxecCm"
      },
      "outputs": [],
      "source": [
        "!gcloud dataproc jobs submit pyspark $NOME_ARQUIVO_SCRIPT --cluster=$CLUSTER_NAME --region=$GCP_REGION --project=$PROJECT_NAME -- $CAMINHO_ARQUIVO_ENTRADA"
      ],
      "id": "NfUS6KWxecCm"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_lzAeVgsDX9"
      },
      "source": [
        " LINK DA PAGINA DE EXECUÇÃO <br>\n",
        " http://leide-elenkarla-m:8088/proxy/application_1674944375475_0003/\n"
      ],
      "id": "s_lzAeVgsDX9"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "-2t2Dq9bht0X",
        "outputId": "60312cbb-8830-4f61-ecf3-d1cd9be89956"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The cluster 'leide-elenkarla' and all attached disks will be deleted.\n",
            "\n",
            "Do you want to continue (Y/n)?  "
          ]
        }
      ],
      "source": [
        "##########    DELETEANDO O CLUSTER ##################\n",
        "\n",
        "!gcloud dataproc clusters delete $CLUSTER_NAME --region=$GCP_REGION --project=$PROJECT_NAME"
      ],
      "id": "-2t2Dq9bht0X"
    }
  ],
  "metadata": {
    "celltoolbar": "Slideshow",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.15"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}